import os
import requests
import json
from datetime import datetime, timedelta, timezone
from dotenv import load_dotenv
import argparse
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- Configuration ---
# Load environment variables from .env automatically
load_dotenv()
API_KEY = os.environ.get("API_KEY")  # Gemini API Key

# It's crucial to set a User-Agent that is descriptive and includes your contact info.
# SEC may block requests with generic user agents.
# Format: "Sample Company Name AdminContact@<sample company domain>.com"
HEADERS = {"User-Agent": "dustinopal 4thgencorei7@gmail.com"}

EDGAR_SUBMISSIONS_URL = "https://data.sec.gov/submissions/CIK{}.json"
EDGAR_FILING_URL = "https://www.sec.gov/Archives/edgar/data/{cik}/{accession_nodash}/{accession}.txt"
GEMINI_API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"

COMPANIES_FILE = os.path.join("scripts", "sp500_companies.json") # Use the JSON file generated by the enrichment script
STATE_FILE = os.path.join("scripts", "run_state.json") # Stores the timestamp of the last run
OUTPUT_DIR = "site"
RELEVANT_FORMS = {"8-K", "10-K", "10-Q"}
ANALYSIS_FORMS = {"10-K", "10-Q", "8-K"}  # Forms to run Gemini analysis on
MAX_WORKERS = 10 # Number of companies to process in parallel
INTERESTING_8K_DESCRIPTIONS = [
    "results of operations",  # Earnings releases
    "financial condition",
    "entry into a material definitive agreement",  # Often M&A related
    "completion of acquisition",
    "merger",
    "asset acquisition",
    "divestiture",
    "regulation fd disclosure",  # Can contain forward-looking statements
]

# Max characters to send to Gemini API to avoid exceeding token limits.
# gemini-pro has a 32k token limit, which is roughly 128k characters. 100k is a safe buffer.
MAX_CHARS_FOR_ANALYSIS = 100000

# Setup basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Gemini Analysis ---
def get_gemini_ma_prompt(text, company_name, form_type):
    """Creates a detailed prompt for M&A analysis."""
    return (
        f"Analyze the following SEC filing text from {company_name} ({form_type}) "
        "from a mergers and acquisitions (M&A) perspective. Focus on identifying any language that suggests "
        "ongoing or future M&A activity, including but not limited to:\n"
        "- Explicit mentions of mergers, acquisitions, divestitures, or strategic partnerships.\n"
        "- Discussions of market consolidation or competitive landscape changes that might motivate M&A.\n"
        "- Significant changes in cash position, debt, or capital allocation strategies pointing towards funding an acquisition.\n"
        "- 'Risk Factors' related to integrating acquired businesses or failing to complete announced transactions.\n\n"
        "Provide a summary in the following format:\n"
        "## M&A Analysis Summary\n\n"
        "**Overall Sentiment:** (e.g., Active, Passive, Speculative, None)\n\n"
        "**Key Findings:**\n"
        "- (Bulleted list of direct evidence or strong indicators)\n\n"
        "**Potential Activity:**\n"
        "- (Bulleted list of any speculative or potential future activities inferred from the text)\n\n"
        "**Direct Quotes:**\n"
        "- (Bulleted list of direct quotes from the filing that support the analysis)\n\n"
        f"--- Filing Text ---\n{text}"
    )

def get_gemini_8k_ma_prompt(text, company_name):
    """Creates a detailed prompt for M&A analysis of an 8-K filing, likely containing an earnings release."""
    return (
        f"Analyze the following Form 8-K filing from {company_name} "
        "from a mergers and acquisitions (M&A) perspective. This filing likely contains an earnings press release (often in Exhibit 99.1). "
        "Focus on the press release content and any forward-looking statements to identify language that suggests "
        "ongoing or future M&A activity, including but not limited to:\n"
        "- Explicit mentions of acquisitions, divestitures, or strategic investments in the earnings summary.\n"
        "- Management commentary on capital allocation priorities (e.g., share buybacks vs. M&A).\n"
        "- Discussion of market consolidation, competitive positioning, or plans to enter new markets via acquisition.\n"
        "- Significant changes in cash, debt, or financial outlook that would support or necessitate M&A.\n\n"
        "Provide a summary in the following format:\n"
        "## M&A Analysis Summary (from 8-K Earnings Release)\n\n"
        "**Overall Sentiment:** (e.g., Active, Passive, Speculative, None)\n\n"
        "**Key Findings:**\n"
        "- (Bulleted list of direct evidence or strong indicators from the press release)\n\n"
        "**Direct Quotes:**\n"
        "- (Bulleted list of direct quotes from the filing that support the analysis)\n\n"
        f"--- 8-K Filing Text ---\n{text}"
    )

def analyze_with_gemini(prompt, company_name):
    """Sends a prompt to the Gemini API for analysis."""
    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [{"parts": [{"text": prompt}]}]
    }
    params = {"key": API_KEY}

    try:
        resp = requests.post(GEMINI_API_URL, headers=headers, params=params, json=payload, timeout=120)
        resp.raise_for_status()  # Raises an HTTPError for bad responses (4xx or 5xx)

        result = resp.json()
        
        candidates = result.get("candidates")
        if not candidates:
            # Handle cases where the model might refuse to answer (e.g., safety settings)
            prompt_feedback = result.get("promptFeedback", {})
            logging.warning(f"Gemini analysis for {company_name} failed. No candidates returned. Feedback: {prompt_feedback}")
            return f"Analysis failed for {company_name}. Reason: No content generated. Feedback: {prompt_feedback}"

        report = candidates[0].get("content", {}).get("parts", [{}])[0].get("text", "")
        return report if report else f"Analysis for {company_name} resulted in an empty report."

    except requests.exceptions.RequestException as e:
        logging.error(f"Gemini API request failed for {company_name}: {e}")
        return f"Analysis failed: Network error connecting to Gemini API. {e}"
    except (KeyError, IndexError) as e:
        logging.error(f"Failed to parse Gemini response for {company_name}: {e}\nResponse: {result}")
        return "Analysis failed: Could not parse Gemini API response."

# --- SEC EDGAR Functions ---
def get_recent_filings(cik):
    """Fetches recent filings for a given CIK from SEC EDGAR."""
    url = EDGAR_SUBMISSIONS_URL.format(cik)
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        return data.get("filings", {}).get("recent", {})
    except requests.exceptions.RequestException as e:
        logging.error(f"Failed to get filings for CIK {cik}: {e}")
        return None
    except json.JSONDecodeError:
        logging.error(f"Failed to decode JSON for CIK {cik}. Response text: {resp.text[:200]}")
        return None

def get_filing_text(cik, accession_no, form, ticker):
    """Downloads the full text of a filing and optionally saves it."""
    accession_no_nodash = accession_no.replace('-', '')
    url = EDGAR_FILING_URL.format(cik=int(cik), accession_nodash=accession_no_nodash, accession=accession_no)
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=30)
        resp.raise_for_status()
        filing_text = resp.text

        # Save the filing text for archival purposes
        filename = f"{ticker}_{form}_{accession_no}.txt"
        filepath = os.path.join(OUTPUT_DIR, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(filing_text)
        logging.info(f"Saved filing to {filepath}")
        
        return filing_text
    except requests.exceptions.RequestException as e:
        logging.error(f"Failed to download filing {accession_no} for {ticker}: {e}")
        return None

# --- State Management ---
def load_last_run_time(state_file):
    """Loads the last run timestamp from the state file."""
    try:
        with open(state_file, 'r') as f:
            state = json.load(f)
            last_run_str = state.get('last_run_utc')
            # Parse the ISO format string back into a timezone-aware datetime object
            return datetime.fromisoformat(last_run_str)
    except (FileNotFoundError, json.JSONDecodeError, KeyError):
        logging.warning("State file not found or invalid. Defaulting to a 1-day lookback period.")
        # Return a timezone-aware datetime object for consistent comparison
        return datetime.now(timezone.utc) - timedelta(days=1)

def save_current_run_time(state_file, run_time):
    """Saves the current run timestamp to the state file."""
    try:
        with open(state_file, 'w') as f:
            # Store the timestamp in ISO format for easy parsing
            state = {'last_run_utc': run_time.isoformat()}
            json.dump(state, f, indent=4)
        logging.info(f"Saved current run timestamp to {state_file}")
    except IOError as e:
        logging.error(f"Could not write to state file {state_file}: {e}")

# --- Report Parsing ---
def parse_report(markdown_text):
    """Parses the Gemini-generated markdown report into a structured dictionary."""
    data = {
        "sentiment": "N/A",
        "findings": [],
        "potential_activity": [],
        "quotes": [],
    }
    if not markdown_text:
        return data

    lines = markdown_text.split('\n')
    current_section = None

    for line in lines:
        line = line.strip()
        lower_line = line.lower()

        if "**overall sentiment:**" in lower_line:
            parts = line.split(':', 1)
            if len(parts) > 1:
                data["sentiment"] = parts[1].strip()
            current_section = None
        elif "**key findings:**" in lower_line:
            current_section = "findings"
        elif "**potential activity:**" in lower_line:
            current_section = "potential_activity"
        elif "**direct quotes:**" in lower_line:
            current_section = "quotes"
        elif line.startswith("- ") and current_section:
            item = line[2:].strip()
            if item and current_section in data:
                data[current_section].append(item)
    return data

# --- Cleanup Utility ---
def cleanup_output_directory(directory):
    """Deletes generated .txt and .md files from the output directory."""
    logging.info(f"--- Running Cleanup Utility for directory: '{directory}' ---")
    if not os.path.isdir(directory):
        logging.warning(f"Output directory '{directory}' not found. Nothing to clean.")
        return

    files_deleted = 0
    for filename in os.listdir(directory):
        if filename.endswith((".txt", ".md")):
            try:
                filepath = os.path.join(directory, filename)
                os.remove(filepath)
                logging.info(f"Deleted: {filename}")
                files_deleted += 1
            except OSError as e:
                logging.error(f"Error deleting file {filename}: {e}")
    
    logging.info(f"--- Cleanup Complete. Deleted {files_deleted} files. ---")


# --- Analysis and Reporting ---
def save_analysis_report(report_text, ticker, document_type_label):
    """Saves a generated analysis report to a file, handling failed analyses."""
    if not report_text or "Analysis failed" in report_text:
        logging.warning(f"Skipping save for failed analysis of {document_type_label} for {ticker}.")
        return

    report_filename = f"{ticker}_{document_type_label}_ma_report.md"
    report_filepath = os.path.join(OUTPUT_DIR, report_filename)
    with open(report_filepath, "w", encoding="utf-8") as out:
        out.write(report_text)
    logging.info(f"Saved {document_type_label} M&A analysis report to {report_filepath}")

# --- Company Processing ---
def process_company(company, cutoff_date):
    """
    Processes a single company: finds recent filings, downloads them, and triggers analysis.
    Returns a list of dictionaries for each recent filing found.
    """
    ticker = company.get("symbol")
    company_name = company.get("name", ticker)
    if not ticker:
        logging.warning(f"Skipping company entry with no symbol: {company}")
        return []

    # The sp500_companies.json file should always have the CIK.
    cik = company.get("cik")
    if not cik:
        logging.error(f"CRITICAL: Could not find CIK for {ticker} in {COMPANIES_FILE}. Please run the enrichment script.")
        return []
    
    # The CIK should already be a zero-padded string from the enrichment script.
    logging.info(f"Found CIK {cik} for {company_name} ({ticker}). Fetching recent filings from SEC...")

    filings = get_recent_filings(cik)
    if not filings:
        # This is the second critical failure point. If this happens for all companies, check the User-Agent header.
        logging.warning(f"No recent filings data returned from SEC for {ticker} (CIK: {cik}). The company may have no recent filings, or the request failed.")
        return []
    
    logging.info(f"Successfully received filing data from SEC for {ticker}. Checking for new documents...")

    # The filings data is a dict of lists, so we need to iterate carefully
    forms = filings.get("form", [])
    filing_dates = filings.get("filingDate", [])
    accession_numbers = filings.get("accessionNumber", [])
    doc_descriptions = filings.get("primaryDocDescription", [])  # Get descriptions
    acceptance_datetimes = filings.get("acceptanceDateTime", []) # Get precise timestamps
    
    company_filings = []

    for i in range(len(forms)):
        form = forms[i]
        if form in RELEVANT_FORMS:
            # Use the precise acceptanceDateTime for filtering
            if i >= len(acceptance_datetimes):
                continue

            acceptance_dt_str = acceptance_datetimes[i]
            # Make string compatible with fromisoformat across Python versions
            if acceptance_dt_str.endswith('Z'):
                acceptance_dt_str = acceptance_dt_str[:-1] + '+00:00'
            
            try:
                filing_dt = datetime.fromisoformat(acceptance_dt_str)
            except (ValueError, TypeError):
                logging.warning(f"Could not parse acceptanceDateTime '{acceptance_dt_str}' for {ticker}. Skipping this filing.")
                continue

            if filing_dt > cutoff_date:
                logging.info(f"Found new filing for {ticker}: {form} filed at {filing_dt.strftime('%Y-%m-%d %H:%M:%S UTC')}")
                
                # --- Loose content filter ---
                should_analyze = False
                if form in ("10-K", "10-Q"):
                    should_analyze = True  # Always analyze quarterly and annual reports
                elif form == "8-K":
                    description = doc_descriptions[i] if i < len(doc_descriptions) else ""
                    if any(keyword in description.lower() for keyword in INTERESTING_8K_DESCRIPTIONS):
                        logging.info(f"Found interesting 8-K for {ticker}: '{description}'")
                        should_analyze = True
                    else:
                        logging.info(f"Skipping analysis for non-relevant 8-K for {ticker}: '{description}'")
                
                if should_analyze:
                    logging.info(f"Analyzing {form} for {ticker}...")
                    accession_no = accession_numbers[i]
                    filing_text = get_filing_text(cik, accession_no, form, ticker)
                    if filing_text:
                        if len(filing_text) > MAX_CHARS_FOR_ANALYSIS:
                            logging.warning(f"Filing for {ticker} is too long, truncating for analysis.")
                            filing_text = filing_text[:MAX_CHARS_FOR_ANALYSIS]

                        prompt = get_gemini_8k_ma_prompt(filing_text, company_name) if form == "8-K" else get_gemini_ma_prompt(filing_text, company_name, form)
                        if prompt:
                            report = analyze_with_gemini(prompt, company_name)
                            if report and "Analysis failed" not in report:
                                save_analysis_report(report, ticker, form)
                                parsed_data = parse_report(report)
                                analysis_result = {
                                    "ticker": ticker, "company_name": company_name, "form": form,
                                    "date": filing_dates[i], "report_path": f"{ticker}_{form}_ma_report.md",
                                    **parsed_data
                                }
                                company_filings.append(analysis_result)
    return company_filings

# --- Main Execution ---
def main():
    """Main script execution function."""
    if not API_KEY:
        logging.error("API_KEY not found in .env file. The script cannot proceed with analysis.")
        return

    try:
        with open(COMPANIES_FILE, "r") as f:
            companies = json.load(f)
    except FileNotFoundError:
        logging.error(f"Company list '{COMPANIES_FILE}' not found. Please run the 'enrich_company_data.py' script first.")
        return
    except json.JSONDecodeError as e:
        logging.error(f"Could not load or parse {COMPANIES_FILE}: {e}")
        return

    # Record the start time of this run to save it later.
    start_time_utc = datetime.now(timezone.utc)
    cutoff_date = load_last_run_time(STATE_FILE)
    logging.info(f"Checking for filings since last run at: {cutoff_date.strftime('%Y-%m-%d %H:%M:%S UTC')}")

    all_recent_filings = []
    logging.info(f"Processing {len(companies)} companies with up to {MAX_WORKERS} parallel workers...")
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # Create a future for each company to be processed
        future_to_company = {
            executor.submit(process_company, company, cutoff_date): company 
            for company in companies
        }

        for future in as_completed(future_to_company):
            company_name = future_to_company[future].get('name', 'Unknown')
            try:
                # Get the result (list of filings) from the completed future
                filings_found = future.result()
                if filings_found:
                    all_recent_filings.extend(filings_found)
            except Exception as exc:
                logging.error(f"An error occurred while processing {company_name}: {exc}", exc_info=True)

    # Update the state file with the timestamp from the beginning of this run for the next execution
    save_current_run_time(STATE_FILE, start_time_utc)

    # Add a final summary message for clarity if no filings were found
    if not all_recent_filings:
        logging.info("--------------------------------------------------")
        logging.info(f"PROCESS COMPLETE: No new, relevant filings found since the last run at {cutoff_date.strftime('%Y-%m-%d %H:%M:%S UTC')}.")
        logging.info("Tip: To test the full analysis pipeline, you can delete 'scripts/run_state.json' to trigger a 1-day lookback on the next run.")
        logging.info("--------------------------------------------------")

    # Output summary of all found filings
    summary_path = os.path.join(OUTPUT_DIR, "analysis_manifest.json")
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(all_recent_filings, f, indent=4)
    logging.info(f"Wrote analysis manifest for {len(all_recent_filings)} reports to {summary_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Analyze SEC filings for M&A signals.")
    parser.add_argument("--clean", action="store_true", help="Clean the output directory of generated reports and filings.")
    args = parser.parse_args()

    if args.clean:
        cleanup_output_directory(OUTPUT_DIR)
    else:
        if not os.path.exists(OUTPUT_DIR):
            os.makedirs(OUTPUT_DIR)
        main()
