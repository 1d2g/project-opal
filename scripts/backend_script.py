import os
import requests
import json
from datetime import datetime, timedelta, timezone, date
from dotenv import load_dotenv
import argparse
import logging
import xml.etree.ElementTree as ET
import time
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- Configuration ---
# Load environment variables from .env automatically
load_dotenv()
API_KEY = os.environ.get("API_KEY")  # Gemini API Key

# It's crucial to set a User-Agent that is descriptive and includes your contact info.
USER_AGENT = "Project Opal anselericson@gmail.com"

EDGAR_FILING_URL = "https://www.sec.gov/Archives/edgar/data/{cik}/{accession_nodash}/{accession}.txt"
GEMINI_API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent"

COMPANIES_FILE = os.path.join("scripts", "sp500_companies.json") # Use the JSON file generated by the enrichment script
STATE_FILE = os.path.join("scripts", "run_state.json") # Stores the timestamp of the last run
OUTPUT_DIR = "site"
REPORTS_SUBDIR_NAME = "reports"
REPORTS_DIR = os.path.join(OUTPUT_DIR, REPORTS_SUBDIR_NAME)
RELEVANT_FORMS = {"8-K", "10-K", "10-Q"}
ANALYSIS_FORMS = {"10-K", "10-Q", "8-K"}  # Forms to run Gemini analysis on
MAX_WORKERS = 8 # Number of filings to process in parallel. Kept under 10 to respect SEC rate limits.

# Max characters to send to Gemini API to avoid exceeding token limits.
MAX_CHARS_FOR_ANALYSIS = 100000

# Setup basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Gemini Analysis ---
def get_gemini_ma_prompt(text, company_name, form_type):
    """Creates a prompt for M&A analysis, tailored for a financial audience."""
    return (
        f"You are a financial analyst specializing in Mergers and Acquisitions. Your task is to analyze the following SEC filing from {company_name} ({form_type}).\n\n"
        "Your analysis should focus exclusively on identifying signals related to potential M&A activity. Look for:\n"
        "- Explicit statements about mergers, acquisitions, divestitures, or strategic investments.\n"
        "- Management discussion on capital allocation priorities, especially regarding M&A.\n"
        "- Significant changes in financial position (e.g., cash reserves, debt issuance) that could facilitate a transaction.\n"
        "- Forward-looking statements or risk factors that mention corporate development or strategic transactions.\n\n"
        "Present your findings in a clear, concise report. Do not use markdown formatting like asterisks or hashtags.\n\n"
        "If you find relevant M&A signals, structure your report as follows:\n"
        "Overall Sentiment: [One of: Active, Speculative, Passive, None]\n"
        "Key Findings:\n"
        "- [A bulleted list of direct evidence of M&A activity.]\n"
        "Potential Activity:\n"
        "- [A bulleted list of any inferred future activities.]\n"
        "Direct Quotes:\n"
        "- [A bulleted list of verbatim quotes supporting your findings.]\n\n"
        "If there is NO M&A-related language in the filing, provide a brief, one-sentence summary of the filing's primary purpose instead. For example: 'This 10-Q is a standard quarterly financial report with no notable M&A discussion.' Do not include the other sections if no M&A activity is found.\n\n"
        f"--- Filing Text ---\n{text}"
    )

def get_gemini_8k_ma_prompt(text, company_name):
    """Creates a prompt for M&A analysis of an 8-K, tailored for a financial audience."""
    return (
        f"You are a financial analyst specializing in Mergers and Acquisitions. Your task is to analyze the following Form 8-K from {company_name}, which likely contains an earnings press release.\n\n"
        "Your analysis should focus exclusively on identifying signals related to potential M&A activity within the press release and management commentary. Look for:\n"
        "- Explicit statements about acquisitions, divestitures, or strategic investments.\n"
        "- Management discussion on capital allocation priorities (e.g., share buybacks vs. M&A).\n"
        "- Forward-looking statements about entering new markets or consolidation.\n"
        "- Significant changes in financial outlook that would support a transaction.\n\n"
        "Present your findings in a clear, concise report. Do not use markdown formatting like asterisks or hashtags.\n\n"
        "If you find relevant M&A signals, structure your report as follows:\n"
        "Overall Sentiment: [One of: Active, Speculative, Passive, None]\n"
        "Key Findings:\n"
        "- [A bulleted list of direct evidence from the press release.]\n"
        "Direct Quotes:\n"
        "- [A bulleted list of verbatim quotes supporting your findings.]\n\n"
        "If there is NO M&A-related language in the filing, provide a brief, one-sentence summary of the filing's primary purpose instead. For example: 'This 8-K announces quarterly earnings with no notable M&A discussion.' Do not include the other sections if no M&A activity is found.\n\n"
        f"--- 8-K Filing Text ---\n{text}"
    )

def analyze_with_gemini(prompt, company_name):
    """Sends a prompt to the Gemini API for analysis, with retry logic for transient errors."""
    headers = {
        "Content-Type": "application/json",
        "x-goog-api-key": API_KEY
    }
    payload = {
        "contents": [{"parts": [{"text": prompt}]}]
    }
    
    attempt = 0
    backoff_factor = 5  # Initial wait time in seconds
    max_wait_time = 120 # 2 minutes

    while True:
        try:
            resp = requests.post(GEMINI_API_URL, headers=headers, json=payload, timeout=120)
            resp.raise_for_status()  # Raises an HTTPError for bad responses (4xx or 5xx)

            result = resp.json()
            
            candidates = result.get("candidates")
            if not candidates:
                prompt_feedback = result.get("promptFeedback", {})
                logging.warning(f"Gemini analysis for {company_name} failed. No candidates returned. Feedback: {prompt_feedback}")
                return f"Analysis failed for {company_name}. Reason: No content generated. Feedback: {prompt_feedback}"

            report = candidates[0].get("content", {}).get("parts", [{}])[0].get("text", "")
            return report if report else f"Analysis for {company_name} resulted in an empty report."

        except requests.exceptions.RequestException as e:
            # Check if the error is a 503 (transient server error) or 429 (rate limit), which are good candidates for a retry.
            if hasattr(e, 'response') and e.response is not None and e.response.status_code in [503, 429]:
                wait_time = min(backoff_factor * (2 ** attempt), max_wait_time)
                logging.warning(f"Gemini API returned {e.response.status_code} for {company_name}. Retrying in {wait_time} seconds... (Attempt {attempt + 1})")
                time.sleep(wait_time)
                attempt += 1
                continue  # Go to the next attempt
            
            # For other request errors (like 4xx) or if retries are exhausted, log and fail.
            logging.error(f"Gemini API request failed for {company_name}: {e}")
            return f"Analysis failed: Network error connecting to Gemini API. {e}"
        except (KeyError, IndexError) as e:
            logging.error(f"Failed to parse Gemini response for {company_name}: {e}\nResponse: {resp.text}")
            return "Analysis failed: Could not parse Gemini API response."

def get_filing_text(cik, accession_no, form, ticker):
    """Downloads the full text of a filing and optionally saves it."""
    accession_no_nodash = accession_no.replace('-', '')
    url = EDGAR_FILING_URL.format(cik=int(cik), accession_nodash=accession_no_nodash, accession=accession_no)
    
    headers = {"User-Agent": USER_AGENT}
    attempt = 0
    backoff_factor = 10  # Initial wait time in seconds, longer for SEC
    max_wait_time = 120 # 2 minutes

    while True:
        try:
            resp = requests.get(url, headers=headers, timeout=30)
            resp.raise_for_status()
            filing_text = resp.text

            filename = f"{ticker}_{form}_{accession_no}.txt"
            filepath = os.path.join(REPORTS_DIR, filename)
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(filing_text)
            logging.info(f"Saved filing to {filepath}")
            
            return filing_text
        except requests.exceptions.RequestException as e:
            # Specifically handle 429 Too Many Requests from SEC
            if hasattr(e, 'response') and e.response is not None and e.response.status_code == 429:
                # SEC may provide a 'Retry-After' header. Use it if available.
                retry_after_header = e.response.headers.get("Retry-After")
                if retry_after_header and retry_after_header.isdigit():
                    wait_time = int(retry_after_header)
                else:
                    # Fallback to exponential backoff if header is missing or invalid
                    wait_time = min(backoff_factor * (2 ** attempt), max_wait_time)
                
                logging.warning(f"SEC returned 429 Too Many Requests for {ticker}. Retrying in {wait_time} seconds... (Attempt {attempt + 1})")
                time.sleep(wait_time)
                attempt += 1
                continue
            
            logging.error(f"Failed to download filing {accession_no} for {ticker}: {e}")
            # For other errors, we break the loop and return None after the first failure.
            return None

# --- State Management ---
def load_last_run_time(state_file):
    """Loads the last run timestamp from the state file."""
    try:
        with open(state_file, 'r') as f:
            state = json.load(f)
            last_run_str = state.get('last_run_utc')
            # Parse the ISO format string back into a timezone-aware datetime object
            return datetime.fromisoformat(last_run_str)
    except (FileNotFoundError, json.JSONDecodeError, KeyError):
        logging.warning("State file not found or invalid. Defaulting to a 7-day lookback period to build a backlog.")
        # Return a timezone-aware datetime object for consistent comparison
        return datetime.now(timezone.utc) - timedelta(days=7)

def save_current_run_time(state_file, run_time):
    """Saves the current run timestamp to the state file."""
    try:
        with open(state_file, 'w') as f:
            # Store the timestamp in ISO format for easy parsing
            state = {'last_run_utc': run_time.isoformat()}
            json.dump(state, f, indent=4)
        logging.info(f"Saved current run timestamp to {state_file}")
    except IOError as e:
        logging.error(f"Could not write to state file {state_file}: {e}")

# --- Report Parsing ---
def parse_report(markdown_text):
    """Parses the Gemini-generated report into a structured dictionary."""
    if not markdown_text:
        return {"sentiment": "N/A", "findings": [], "potential_activity": [], "quotes": []}

    # Handle the case where Gemini provides a simple summary instead of a structured report
    if "Overall Sentiment:" not in markdown_text:
        return {
            "sentiment": "None",
            "findings": [markdown_text.strip()], # Put the summary sentence as the only finding
            "potential_activity": [],
            "quotes": []
        }

    data = {
        "sentiment": "N/A",
        "findings": [],
        "potential_activity": [],
        "quotes": [],
    }

    lines = markdown_text.split('\n')
    current_section = None

    for line in lines:
        line = line.strip()
        # Use .startswith() for more precise matching without being case-sensitive or format-sensitive
        if line.startswith("Overall Sentiment:"):
            parts = line.split(':', 1)
            if len(parts) > 1:
                data["sentiment"] = parts[1].strip()
            current_section = None
        elif line.startswith("Key Findings:"):
            current_section = "findings"
        elif line.startswith("Potential Activity:"):
            current_section = "potential_activity"
        elif line.startswith("Direct Quotes:"):
            current_section = "quotes"
        elif line.startswith("- ") and current_section:
            item = line[2:].strip()
            if item and current_section in data:
                data[current_section].append(item)
    return data

# --- Cleanup Utility ---
def cleanup_output_directory(directory):
    """Deletes generated analysis files from their respective directories."""
    logging.info(f"--- Running Cleanup Utility for directory: '{directory}' ---")
    
    files_deleted = 0
    reports_dir = os.path.join(directory, REPORTS_SUBDIR_NAME)

    # Clean the reports subdirectory
    if os.path.isdir(reports_dir):
        logging.info(f"Cleaning reports subdirectory: '{reports_dir}'")
        for filename in os.listdir(reports_dir):
            if filename.endswith((".txt", ".md")):
                try:
                    filepath = os.path.join(reports_dir, filename)
                    os.remove(filepath)
                    logging.info(f"Deleted: {os.path.join(REPORTS_SUBDIR_NAME, filename)}")
                    files_deleted += 1
                except OSError as e:
                    logging.error(f"Error deleting file {filename}: {e}")
    
    # Clean manifest from the main site directory
    manifest_path = os.path.join(directory, "analysis_manifest.json")
    if os.path.exists(manifest_path):
        try:
            os.remove(manifest_path)
            logging.info(f"Deleted: analysis_manifest.json")
            files_deleted += 1
        except OSError as e:
            logging.error(f"Error deleting file analysis_manifest.json: {e}")

    # Also clean the run state from the parent scripts directory
    if os.path.exists(STATE_FILE):
        try:
            os.remove(STATE_FILE)
            logging.info(f"Deleted: {os.path.basename(STATE_FILE)}")
            files_deleted += 1
        except OSError as e:
            logging.error(f"Error deleting file {os.path.basename(STATE_FILE)}: {e}")

    logging.info(f"--- Cleanup Complete. Deleted {files_deleted} files. ---")


# --- Analysis and Reporting ---
def save_analysis_report(report_text, ticker, document_type_label):
    """Saves a generated analysis report to a file, handling failed analyses."""
    if not report_text or "Analysis failed" in report_text:
        logging.warning(f"Skipping save for failed analysis of {document_type_label} for {ticker}.")
        return

    report_filename = f"{ticker}_{document_type_label}_ma_report.md"
    report_filepath = os.path.join(REPORTS_DIR, report_filename)
    with open(report_filepath, "w", encoding="utf-8") as out:
        out.write(report_text)
    logging.info(f"Saved {document_type_label} M&A analysis report to {report_filepath}")

# --- New Filing Discovery Workflow ---
def get_sp500_cik_map():
    """Loads the company JSON and returns a dictionary mapping CIK to company info."""
    try:
        with open(COMPANIES_FILE, "r") as f:
            companies = json.load(f)
        # Pad CIKs to 10 digits to match SEC master index format
        return {str(c['cik']).zfill(10): {'ticker': c['symbol'], 'name': c['name']} for c in companies if 'cik' in c and 'symbol' in c}
    except (FileNotFoundError, json.JSONDecodeError) as e:
        logging.error(f"CRITICAL: Could not load or parse {COMPANIES_FILE}: {e}. Cannot proceed.")
        return {}

def discover_new_filings_from_rss(session, sp500_map, cutoff_date):
    """
    Discovers new filings from the SEC's RSS feed, which is the most reliable
    source for recent filings. This replaces older, less reliable methods.
    """
    logging.info("Fetching recent filings from SEC RSS feed...")
    try:
        # We can pre-filter by form type in the URL for efficiency
        forms_query = "".join([f"&type={f}" for f in RELEVANT_FORMS])
        url = f"https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent{forms_query}&count=100&output=atom"
        
        response = session.get(url)
        response.raise_for_status()
        
        # Register namespace to properly parse the Atom feed
        ns = {'atom': 'http://www.w3.org/2005/Atom'}
        root = ET.fromstring(response.content)
        
        filings_to_process = []
        
        for entry in root.findall('atom:entry', ns):
            updated_str = entry.find('atom:updated', ns).text
            filing_datetime = datetime.fromisoformat(updated_str)
            
            if filing_datetime <= cutoff_date:
                continue # Skip filings that are not new
            
            # Extract CIK from title, e.g., "8-K - Alphabet Inc. (0001652044) (Filer)"
            title = entry.find('atom:title', ns).text
            cik_match = re.search(r'\((\d+)\)', title)
            if not cik_match:
                continue
            
            cik_padded = cik_match.group(1).zfill(10)
            
            if cik_padded in sp500_map:
                form_type = entry.find('atom:category', ns).attrib.get('term')
                link = entry.find('atom:link', ns).attrib.get('href')
                
                # Extract accession number from link, e.g., .../0001652044-24-000034-index.htm
                accession_match = re.search(r'(\d{10}-\d{2}-\d{6})', link)
                if not accession_match:
                    continue
                
                accession_no = accession_match.group(1)
                
                filing_details = {
                    'cik': cik_padded, 'form': form_type,
                    'file_path': f"edgar/data/{int(cik_padded)}/{accession_no.replace('-', '')}/{accession_no}.txt",
                    **sp500_map[cik_padded],
                    'date_filed': filing_datetime.strftime('%Y-%m-%d')
                }
                filings_to_process.append(filing_details)
        
        return filings_to_process
        
    except (requests.exceptions.RequestException, ET.ParseError, KeyError, AttributeError) as e:
        logging.error(f"Failed to fetch or parse SEC RSS feed: {e}")
        return []

def analyze_filing(filing_info):
    """
    Takes a dictionary with filing info, performs analysis, and returns a structured result.
    """
    ticker = filing_info['ticker']
    company_name = filing_info['name']
    form = filing_info['form']
    cik = filing_info['cik']
    filing_date = filing_info['date_filed']
    
    # Extract accession number from file path
    accession_no = os.path.basename(filing_info['file_path']).replace('.txt', '')

    logging.info(f"Analyzing {form} for {company_name} ({ticker}) filed on {filing_date}...")
    filing_text = get_filing_text(cik, accession_no, form, ticker)
    if not filing_text:
        logging.warning(f"Filing download failed for {ticker} {form}.")
        return None # Indicates failure

    if len(filing_text) > MAX_CHARS_FOR_ANALYSIS:
        logging.warning(f"Filing for {ticker} is too long, truncating for analysis.")
        filing_text = filing_text[:MAX_CHARS_FOR_ANALYSIS]

    prompt = get_gemini_8k_ma_prompt(filing_text, company_name) if form == "8-K" else get_gemini_ma_prompt(filing_text, company_name, form)
    if not prompt:
        return None # Should not happen, but good practice

    report = analyze_with_gemini(prompt, company_name)
    if not report or "Analysis failed" in report:
        logging.warning(f"Gemini analysis failed for {ticker} {form}.")
        return None # Indicates failure

    save_analysis_report(report, ticker, form)
    parsed_data = parse_report(report)
    return {
        "ticker": ticker, "company_name": company_name, "form": form,
        "date": filing_date, "report_path": f"{REPORTS_SUBDIR_NAME}/{ticker}_{form}_ma_report.md",
        "cik": cik,
        "accession_no": accession_no,
        **parsed_data
    }

# --- Main Execution ---
def main():
    """Main script execution function."""
    if not API_KEY:
        logging.error("API_KEY not found in .env file. The script cannot proceed with analysis.")
        return

    sp500_map = get_sp500_cik_map()
    if not sp500_map:
        return

    # Record the start time of this run to save it later.
    start_time_utc = datetime.now(timezone.utc)
    cutoff_date = load_last_run_time(STATE_FILE)
    logging.info(f"Checking for filings since last run at: {cutoff_date.strftime('%Y-%m-%d %H:%M:%S UTC')}")

    # --- Discover Filings ---
    all_filings_to_process = []
    with requests.Session() as session:
        session.headers.update({"User-Agent": USER_AGENT})
        all_filings_to_process = discover_new_filings_from_rss(session, sp500_map, cutoff_date)

    # De-duplicate just in case the feed has duplicates
    unique_filings = {os.path.basename(f['file_path']): f for f in all_filings_to_process}
    final_filings_to_process = list(unique_filings.values())
    logging.info(f"Found {len(final_filings_to_process)} new, unique, relevant filings to analyze.")

    # --- Analyze Filings ---
    newly_analyzed_reports = []
    processing_failures = 0
    if final_filings_to_process:
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_filing = {executor.submit(analyze_filing, f): f for f in final_filings_to_process}
            for future in as_completed(future_to_filing):
                filing_info = future_to_filing[future]
                try:
                    result = future.result()
                    if result:
                        newly_analyzed_reports.append(result)
                    else:
                        processing_failures += 1 # A result of None indicates failure
                except Exception as exc:
                    processing_failures += 1
                    logging.error(f"An error occurred analyzing {filing_info.get('ticker')}: {exc}", exc_info=True)

    # --- Manifest Generation ---
    # Load the existing manifest to preserve old reports
    manifest_path = os.path.join(OUTPUT_DIR, "analysis_manifest.json")
    existing_reports = []
    if os.path.exists(manifest_path):
        try:
            with open(manifest_path, "r", encoding="utf-8") as f:
                # Handle empty file case
                content = f.read()
                if content:
                    existing_reports = json.loads(content)
        except (json.JSONDecodeError, FileNotFoundError):
            logging.warning(f"Could not read or parse existing manifest at {manifest_path}. A new one will be created.")
            existing_reports = []

    # Combine new reports with existing ones, using a dictionary to handle uniqueness based on report path.
    combined_reports = {report['report_path']: report for report in existing_reports}
    for report in newly_analyzed_reports:
        combined_reports[report['report_path']] = report

    # Convert back to a list and sort by date (most recent first)
    final_manifest = sorted(list(combined_reports.values()), key=lambda r: r.get('date', ''), reverse=True)

    # --- State Saving ---
    # Only update the run state if there were no errors during processing.
    # This ensures that any filings that failed to download or analyze will be retried on the next run.
    if processing_failures == 0:
        save_current_run_time(STATE_FILE, start_time_utc)
    else:
        logging.warning(f"Detected {processing_failures} processing failures. The run state will NOT be updated, allowing for a retry on the next run.")

    if not newly_analyzed_reports and not final_filings_to_process:
        logging.info("--------------------------------------------------")
        logging.info(f"PROCESS COMPLETE: No new, relevant filings found since the last run at {cutoff_date.strftime('%Y-%m-%d %H:%M:%S UTC')}.")
        logging.info("The analysis manifest has been updated with any existing reports.")
        logging.info("--------------------------------------------------")

    # Overwrite the manifest with the new combined and sorted list
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(final_manifest, f, indent=4)
    logging.info(f"Wrote analysis manifest for {len(final_manifest)} total reports to {manifest_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Analyze SEC filings for M&A signals.")
    parser.add_argument("--clean", action="store_true", help="Clean the output directory of generated reports and filings.")
    args = parser.parse_args()

    if args.clean:
        cleanup_output_directory(OUTPUT_DIR)
    else:
        if not os.path.exists(OUTPUT_DIR):
            os.makedirs(OUTPUT_DIR)
        if not os.path.exists(REPORTS_DIR):
            os.makedirs(REPORTS_DIR)
        main()
