import os
import requests
import json
from datetime import datetime, timedelta, timezone, date
from dotenv import load_dotenv
import argparse
import logging
import xml.etree.ElementTree as ET
import time
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- Configuration ---
# Load environment variables from .env automatically
load_dotenv()
API_KEY = os.environ.get("API_KEY")  # Gemini API Key

# It's crucial to set a User-Agent that is descriptive and includes your contact info.
USER_AGENT = "Project Opal anselericson@gmail.com"

EDGAR_FILING_URL = "https://www.sec.gov/Archives/edgar/data/{cik}/{accession_nodash}/{accession}.txt"
GEMINI_API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent"

COMPANIES_FILE = os.path.join("scripts", "sp500_companies.json") # Use the JSON file generated by the enrichment script
STATE_FILE = os.path.join("scripts", "run_state.json") # Stores the timestamp of the last run
OUTPUT_DIR = "site"
RELEVANT_FORMS = {"8-K", "10-K", "10-Q"}
ANALYSIS_FORMS = {"10-K", "10-Q", "8-K"}  # Forms to run Gemini analysis on
MAX_WORKERS = 8 # Number of filings to process in parallel. Kept under 10 to respect SEC rate limits.

# Max characters to send to Gemini API to avoid exceeding token limits.
MAX_CHARS_FOR_ANALYSIS = 100000

# Setup basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Gemini Analysis ---
def get_gemini_ma_prompt(text, company_name, form_type):
    """Creates a detailed prompt for M&A analysis."""
    return (
        f"Analyze the following SEC filing text from {company_name} ({form_type}) "
        "from a mergers and acquisitions (M&A) perspective. Focus on identifying any language that suggests "
        "ongoing or future M&A activity, including but not limited to:\n"
        "- Explicit mentions of mergers, acquisitions, divestitures, or strategic partnerships.\n"
        "- Discussions of market consolidation or competitive landscape changes that might motivate M&A.\n"
        "- Significant changes in cash position, debt, or capital allocation strategies pointing towards funding an acquisition.\n"
        "- 'Risk Factors' related to integrating acquired businesses or failing to complete announced transactions.\n\n"
        "Provide a summary in the following format:\n"
        "## M&A Analysis Summary\n\n"
        "**Overall Sentiment:** (e.g., Active, Passive, Speculative, None)\n\n"
        "**Key Findings:**\n"
        "- (Bulleted list of direct evidence or strong indicators)\n\n"
        "**Potential Activity:**\n"
        "- (Bulleted list of any speculative or potential future activities inferred from the text)\n\n"
        "**Direct Quotes:**\n"
        "- (Bulleted list of direct quotes from the filing that support the analysis)\n\n"
        f"--- Filing Text ---\n{text}"
    )

def get_gemini_8k_ma_prompt(text, company_name):
    """Creates a detailed prompt for M&A analysis of an 8-K filing, likely containing an earnings release."""
    return (
        f"Analyze the following Form 8-K filing from {company_name} "
        "from a mergers and acquisitions (M&A) perspective. This filing likely contains an earnings press release (often in Exhibit 99.1). "
        "Focus on the press release content and any forward-looking statements to identify language that suggests "
        "ongoing or future M&A activity, including but not limited to:\n"
        "- Explicit mentions of acquisitions, divestitures, or strategic investments in the earnings summary.\n"
        "- Management commentary on capital allocation priorities (e.g., share buybacks vs. M&A).\n"
        "- Discussion of market consolidation, competitive positioning, or plans to enter new markets via acquisition.\n"
        "- Significant changes in cash, debt, or financial outlook that would support or necessitate M&A.\n\n"
        "Provide a summary in the following format:\n"
        "## M&A Analysis Summary (from 8-K Earnings Release)\n\n"
        "**Overall Sentiment:** (e.g., Active, Passive, Speculative, None)\n\n"
        "**Key Findings:**\n"
        "- (Bulleted list of direct evidence or strong indicators from the press release)\n\n"
        "**Direct Quotes:**\n"
        "- (Bulleted list of direct quotes from the filing that support the analysis)\n\n"
        f"--- 8-K Filing Text ---\n{text}"
    )

def analyze_with_gemini(prompt, company_name):
    """Sends a prompt to the Gemini API for analysis, with retry logic for transient errors."""
    headers = {
        "Content-Type": "application/json",
        "x-goog-api-key": API_KEY
    }
    payload = {
        "contents": [{"parts": [{"text": prompt}]}]
    }
    
    max_retries = 3
    backoff_factor = 5  # Initial wait time in seconds

    for attempt in range(max_retries):
        try:
            resp = requests.post(GEMINI_API_URL, headers=headers, json=payload, timeout=120)
            resp.raise_for_status()  # Raises an HTTPError for bad responses (4xx or 5xx)

            result = resp.json()
            
            candidates = result.get("candidates")
            if not candidates:
                prompt_feedback = result.get("promptFeedback", {})
                logging.warning(f"Gemini analysis for {company_name} failed. No candidates returned. Feedback: {prompt_feedback}")
                return f"Analysis failed for {company_name}. Reason: No content generated. Feedback: {prompt_feedback}"

            report = candidates[0].get("content", {}).get("parts", [{}])[0].get("text", "")
            return report if report else f"Analysis for {company_name} resulted in an empty report."

        except requests.exceptions.RequestException as e:
            # Check if the error is a 503 Service Unavailable, which is a good candidate for a retry.
            if hasattr(e, 'response') and e.response is not None and e.response.status_code == 503:
                if attempt < max_retries - 1:
                    wait_time = backoff_factor * (2 ** attempt)  # Exponential backoff
                    logging.warning(f"Gemini API returned 503 Service Unavailable for {company_name}. Retrying in {wait_time} seconds... (Attempt {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue  # Go to the next attempt
            
            # For other request errors (like 4xx) or if retries are exhausted, log and fail.
            logging.error(f"Gemini API request failed for {company_name} (attempt {attempt + 1}/{max_retries}): {e}")
            return f"Analysis failed: Network error connecting to Gemini API. {e}"
        except (KeyError, IndexError) as e:
            logging.error(f"Failed to parse Gemini response for {company_name}: {e}\nResponse: {resp.text}")
            return "Analysis failed: Could not parse Gemini API response."
            
    return "Analysis failed: All retry attempts were unsuccessful."

def get_filing_text(cik, accession_no, form, ticker):
    """Downloads the full text of a filing and optionally saves it."""
    accession_no_nodash = accession_no.replace('-', '')
    url = EDGAR_FILING_URL.format(cik=int(cik), accession_nodash=accession_no_nodash, accession=accession_no)
    
    headers = {"User-Agent": USER_AGENT}
    try:
        resp = requests.get(url, headers=headers, timeout=30)
        resp.raise_for_status()
        filing_text = resp.text

        filename = f"{ticker}_{form}_{accession_no}.txt"
        filepath = os.path.join(OUTPUT_DIR, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(filing_text)
        logging.info(f"Saved filing to {filepath}")
        
        return filing_text
    except requests.exceptions.RequestException as e:
        logging.error(f"Failed to download filing {accession_no} for {ticker}: {e}")
        return None

# --- State Management ---
def load_last_run_time(state_file):
    """Loads the last run timestamp from the state file."""
    try:
        with open(state_file, 'r') as f:
            state = json.load(f)
            last_run_str = state.get('last_run_utc')
            # Parse the ISO format string back into a timezone-aware datetime object
            return datetime.fromisoformat(last_run_str)
    except (FileNotFoundError, json.JSONDecodeError, KeyError):
        logging.warning("State file not found or invalid. Defaulting to a 7-day lookback period to build a backlog.")
        # Return a timezone-aware datetime object for consistent comparison
        return datetime.now(timezone.utc) - timedelta(days=7)

def save_current_run_time(state_file, run_time):
    """Saves the current run timestamp to the state file."""
    try:
        with open(state_file, 'w') as f:
            # Store the timestamp in ISO format for easy parsing
            state = {'last_run_utc': run_time.isoformat()}
            json.dump(state, f, indent=4)
        logging.info(f"Saved current run timestamp to {state_file}")
    except IOError as e:
        logging.error(f"Could not write to state file {state_file}: {e}")

# --- Report Parsing ---
def parse_report(markdown_text):
    """Parses the Gemini-generated markdown report into a structured dictionary."""
    data = {
        "sentiment": "N/A",
        "findings": [],
        "potential_activity": [],
        "quotes": [],
    }
    if not markdown_text:
        return data

    lines = markdown_text.split('\n')
    current_section = None

    for line in lines:
        line = line.strip()
        lower_line = line.lower()

        if "**overall sentiment:**" in lower_line:
            parts = line.split(':', 1)
            if len(parts) > 1:
                data["sentiment"] = parts[1].strip()
            current_section = None
        elif "**key findings:**" in lower_line:
            current_section = "findings"
        elif "**potential activity:**" in lower_line:
            current_section = "potential_activity"
        elif "**direct quotes:**" in lower_line:
            current_section = "quotes"
        elif line.startswith("- ") and current_section:
            item = line[2:].strip()
            if item and current_section in data:
                data[current_section].append(item)
    return data

# --- Cleanup Utility ---
def cleanup_output_directory(directory):
    """Deletes generated analysis files (.txt, .md, .json) from the output directory."""
    logging.info(f"--- Running Cleanup Utility for directory: '{directory}' ---")
    if not os.path.isdir(directory):
        logging.warning(f"Output directory '{directory}' not found. Nothing to clean up.")
        return

    files_deleted = 0
    # Define a list of files to delete, including the manifest
    files_to_delete = [f for f in os.listdir(directory) if f.endswith((".txt", ".md"))]
    files_to_delete.append("analysis_manifest.json")
    # Also clean the run state from the parent scripts directory
    if os.path.exists(STATE_FILE):
        try:
            os.remove(STATE_FILE)
            logging.info(f"Deleted: {os.path.basename(STATE_FILE)}")
            files_deleted += 1
        except OSError as e:
            logging.error(f"Error deleting file {os.path.basename(STATE_FILE)}: {e}")

    for filename in files_to_delete:
        filepath = os.path.join(directory, filename)
        if os.path.isfile(filepath): # Check if it's a file before trying to delete
            try:
                os.remove(filepath)
                logging.info(f"Deleted: {filename}")
                files_deleted += 1
            except OSError as e:
                logging.error(f"Error deleting file {filename}: {e}")
    logging.info(f"--- Cleanup Complete. Deleted {files_deleted} files. ---")


# --- Analysis and Reporting ---
def save_analysis_report(report_text, ticker, document_type_label):
    """Saves a generated analysis report to a file, handling failed analyses."""
    if not report_text or "Analysis failed" in report_text:
        logging.warning(f"Skipping save for failed analysis of {document_type_label} for {ticker}.")
        return

    report_filename = f"{ticker}_{document_type_label}_ma_report.md"
    report_filepath = os.path.join(OUTPUT_DIR, report_filename)
    with open(report_filepath, "w", encoding="utf-8") as out:
        out.write(report_text)
    logging.info(f"Saved {document_type_label} M&A analysis report to {report_filepath}")

# --- New Filing Discovery Workflow ---
def get_sp500_cik_map():
    """Loads the company JSON and returns a dictionary mapping CIK to company info."""
    try:
        with open(COMPANIES_FILE, "r") as f:
            companies = json.load(f)
        # Pad CIKs to 10 digits to match SEC master index format
        return {str(c['cik']).zfill(10): {'ticker': c['symbol'], 'name': c['name']} for c in companies if 'cik' in c and 'symbol' in c}
    except (FileNotFoundError, json.JSONDecodeError) as e:
        logging.error(f"CRITICAL: Could not load or parse {COMPANIES_FILE}: {e}. Cannot proceed.")
        return {}

def discover_new_filings_from_rss(session, sp500_map, cutoff_date):
    """
    Discovers new filings from the SEC's RSS feed, which is the most reliable
    source for recent filings. This replaces older, less reliable methods.
    """
    logging.info("Fetching recent filings from SEC RSS feed...")
    try:
        # We can pre-filter by form type in the URL for efficiency
        forms_query = "".join([f"&type={f}" for f in RELEVANT_FORMS])
        url = f"https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent{forms_query}&count=100&output=atom"
        
        response = session.get(url)
        response.raise_for_status()
        
        # Register namespace to properly parse the Atom feed
        ns = {'atom': 'http://www.w3.org/2005/Atom'}
        root = ET.fromstring(response.content)
        
        filings_to_process = []
        
        for entry in root.findall('atom:entry', ns):
            updated_str = entry.find('atom:updated', ns).text
            filing_datetime = datetime.fromisoformat(updated_str)
            
            if filing_datetime <= cutoff_date:
                continue # Skip filings that are not new
            
            # Extract CIK from title, e.g., "8-K - Alphabet Inc. (0001652044) (Filer)"
            title = entry.find('atom:title', ns).text
            cik_match = re.search(r'\((\d+)\)', title)
            if not cik_match:
                continue
            
            cik_padded = cik_match.group(1).zfill(10)
            
            if cik_padded in sp500_map:
                form_type = entry.find('atom:category', ns).attrib.get('term')
                link = entry.find('atom:link', ns).attrib.get('href')
                
                # Extract accession number from link, e.g., .../0001652044-24-000034-index.htm
                accession_match = re.search(r'(\d{10}-\d{2}-\d{6})', link)
                if not accession_match:
                    continue
                
                accession_no = accession_match.group(1)
                
                filing_details = {
                    'cik': cik_padded, 'form': form_type,
                    'file_path': f"edgar/data/{int(cik_padded)}/{accession_no.replace('-', '')}/{accession_no}.txt",
                    **sp500_map[cik_padded],
                    'date_filed': filing_datetime.strftime('%Y-%m-%d')
                }
                filings_to_process.append(filing_details)
        
        return filings_to_process
        
    except (requests.exceptions.RequestException, ET.ParseError, KeyError, AttributeError) as e:
        logging.error(f"Failed to fetch or parse SEC RSS feed: {e}")
        return []

def analyze_filing(filing_info):
    """
    Takes a dictionary with filing info, performs analysis, and returns a structured result.
    """
    ticker = filing_info['ticker']
    company_name = filing_info['name']
    form = filing_info['form']
    cik = filing_info['cik']
    filing_date = filing_info['date_filed']
    
    # Extract accession number from file path
    accession_no = os.path.basename(filing_info['file_path']).replace('.txt', '')

    logging.info(f"Analyzing {form} for {company_name} ({ticker}) filed on {filing_date}...")
    filing_text = get_filing_text(cik, accession_no, form, ticker)
    if not filing_text:
        logging.warning(f"Filing download failed for {ticker} {form}.")
        return None # Indicates failure

    if len(filing_text) > MAX_CHARS_FOR_ANALYSIS:
        logging.warning(f"Filing for {ticker} is too long, truncating for analysis.")
        filing_text = filing_text[:MAX_CHARS_FOR_ANALYSIS]

    prompt = get_gemini_8k_ma_prompt(filing_text, company_name) if form == "8-K" else get_gemini_ma_prompt(filing_text, company_name, form)
    if not prompt:
        return None # Should not happen, but good practice

    report = analyze_with_gemini(prompt, company_name)
    if not report or "Analysis failed" in report:
        logging.warning(f"Gemini analysis failed for {ticker} {form}.")
        return None # Indicates failure

    save_analysis_report(report, ticker, form)
    parsed_data = parse_report(report)
    return {
        "ticker": ticker, "company_name": company_name, "form": form,
        "date": filing_date, "report_path": f"{ticker}_{form}_ma_report.md",
        **parsed_data
    }

# --- Main Execution ---
def main():
    """Main script execution function."""
    if not API_KEY:
        logging.error("API_KEY not found in .env file. The script cannot proceed with analysis.")
        return

    sp500_map = get_sp500_cik_map()
    if not sp500_map:
        return

    # Record the start time of this run to save it later.
    start_time_utc = datetime.now(timezone.utc)
    cutoff_date = load_last_run_time(STATE_FILE)
    logging.info(f"Checking for filings since last run at: {cutoff_date.strftime('%Y-%m-%d %H:%M:%S UTC')}")

    # --- Discover Filings ---
    all_filings_to_process = []
    with requests.Session() as session:
        session.headers.update({"User-Agent": USER_AGENT})
        all_filings_to_process = discover_new_filings_from_rss(session, sp500_map, cutoff_date)

    # De-duplicate just in case the feed has duplicates
    unique_filings = {os.path.basename(f['file_path']): f for f in all_filings_to_process}
    final_filings_to_process = list(unique_filings.values())
    logging.info(f"Found {len(final_filings_to_process)} new, unique, relevant filings to analyze.")

    # --- Analyze Filings ---
    newly_analyzed_reports = []
    processing_failures = 0
    if final_filings_to_process:
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_filing = {executor.submit(analyze_filing, f): f for f in final_filings_to_process}
            for future in as_completed(future_to_filing):
                filing_info = future_to_filing[future]
                try:
                    result = future.result()
                    if result:
                        newly_analyzed_reports.append(result)
                    else:
                        processing_failures += 1 # A result of None indicates failure
                except Exception as exc:
                    processing_failures += 1
                    logging.error(f"An error occurred analyzing {filing_info.get('ticker')}: {exc}", exc_info=True)

    # --- Manifest Generation ---
    # Load the existing manifest to preserve old reports
    manifest_path = os.path.join(OUTPUT_DIR, "analysis_manifest.json")
    existing_reports = []
    if os.path.exists(manifest_path):
        try:
            with open(manifest_path, "r", encoding="utf-8") as f:
                # Handle empty file case
                content = f.read()
                if content:
                    existing_reports = json.loads(content)
        except (json.JSONDecodeError, FileNotFoundError):
            logging.warning(f"Could not read or parse existing manifest at {manifest_path}. A new one will be created.")
            existing_reports = []

    # Combine new reports with existing ones, using a dictionary to handle uniqueness based on report path.
    combined_reports = {report['report_path']: report for report in existing_reports}
    for report in newly_analyzed_reports:
        combined_reports[report['report_path']] = report

    # Convert back to a list and sort by date (most recent first)
    final_manifest = sorted(list(combined_reports.values()), key=lambda r: r.get('date', ''), reverse=True)

    # --- State Saving ---
    # Only update the run state if there were no errors during processing.
    # This ensures that any filings that failed to download or analyze will be retried on the next run.
    if processing_failures == 0:
        save_current_run_time(STATE_FILE, start_time_utc)
    else:
        logging.warning(f"Detected {processing_failures} processing failures. The run state will NOT be updated, allowing for a retry on the next run.")

    if not newly_analyzed_reports and not final_filings_to_process:
        logging.info("--------------------------------------------------")
        logging.info(f"PROCESS COMPLETE: No new, relevant filings found since the last run at {cutoff_date.strftime('%Y-%m-%d %H:%M:%S UTC')}.")
        logging.info("The analysis manifest has been updated with any existing reports.")
        logging.info("--------------------------------------------------")

    # Overwrite the manifest with the new combined and sorted list
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(final_manifest, f, indent=4)
    logging.info(f"Wrote analysis manifest for {len(final_manifest)} total reports to {manifest_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Analyze SEC filings for M&A signals.")
    parser.add_argument("--clean", action="store_true", help="Clean the output directory of generated reports and filings.")
    args = parser.parse_args()

    if args.clean:
        cleanup_output_directory(OUTPUT_DIR)
    else:
        if not os.path.exists(OUTPUT_DIR):
            os.makedirs(OUTPUT_DIR)
        main()
